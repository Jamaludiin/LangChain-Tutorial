# This script demonstrates how to get an AI-generated response, chunk the message, and explore its attributes. ğŸš€
# working

import os
from groq import Groq
from langchain_groq import ChatGroq
from langchain_core.messages import AIMessageChunk
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Get Groq API key from environment variables
groq_api_key = os.getenv('GROQ_API_KEY')

# Initialize the Groq chat model
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.7,
    max_tokens=32767,
    timeout=10,
    max_retries=2,
)

# Define a user prompt
user_prompt = "Explain the importance of AI in modern business."

# Get the AI response
response = llm.invoke(user_prompt)

# Simulating chunked response (Splitting content into chunks)
chunks = response.content.split(". ")  # Split by sentences for simplicity

# Creating an AIMessageChunk object
ai_message_chunk = AIMessageChunk(
    content=chunks[0],  # Taking the first sentence as a chunk
    response_metadata={"token_usage": 20, "model": "llama-3.3-70b"},
    id="chunk_001",
    name="AI Chunk Message"
)

# ğŸ–¨ Printing the outputs with clear formatting
print("\nğŸ”¹ AIMessageChunk Example:")
print("ğŸŸ¢ Content:", ai_message_chunk.content)
print("ğŸŸ¢ ID:", ai_message_chunk.id)
print("ğŸŸ¢ Name:", ai_message_chunk.name)
print("ğŸŸ¢ Response Metadata:", ai_message_chunk.response_metadata)

# Using pretty_print() for formatted output
print("\nğŸ”¹ Pretty Print:")
ai_message_chunk.pretty_print()

# Using pretty_repr() for HTML output
print("\nğŸ”¹ Pretty Representation (HTML Format):")
print(ai_message_chunk.pretty_repr(html=True))




"""
ğŸ“ What This Example Covers
âœ… Uses ChatGroq to generate a response
âœ… Creates an AIMessageChunk instance
âœ… Demonstrates attributes like content, response_metadata, and pretty_print()
âœ… Formatted output for clarity
"""